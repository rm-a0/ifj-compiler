\documentclass[12pt,a4paper]{article}
\usepackage{setspace} % Add to the preamble
\usepackage{graphicx} % For including images
%\usepackage{geometry} % Adjust margins
\usepackage{amsmath}  % For math symbols
\usepackage{rotating}
\usepackage{array}
\usepackage{hyperref} % For hyperlinks
\usepackage{tocloft}  % For customizing table of contents
\usepackage[left=2cm, top=3cm, text={17cm, 24cm}]{geometry}
\usepackage[backend=bibtex]{biblatex}
\usepackage{multirow}
\usepackage{array}
\addbibresource{references.bib}

% Removed duplicate geometry specification
% \geometry{margin=1in}

\begin{document}
\begin{titlepage}
    \begin{center}
        \includegraphics[width=0.6\linewidth]{vut_logo.png} \\

        \vspace{\stretch{0.382}}

        \huge{Projektová dokumentace} \\
        \Large{\textbf{Implementace překladače imperativního jazyka IFJ24}} \\
        
       \vspace{1cm}
       \large{Tým \textbf{xrepcim00}, TRP-izp variant} \\
       \large Extensions: \textbf{FUNEXP}

        
        \vspace{\stretch{0.618}}
    \end{center}

    \begin{minipage}{0.5 \textwidth}
        {\Large \today}
    \end{minipage}
    \hspace{-2 cm}
    \begin{minipage}[r]{0.5 \textwidth}
            \large
            \begin{tabular}{l l l}
                \textbf{Michal Repčík} & \textbf{(xrepcim00)} & \quad 28\,\% \\
                Alex Marinica & (xmarina00) & \quad 22\,\% \\
                Šimon Bobko & (xbobkos00) & \quad 28\,\% \\
                Martin Kandera & (xkande00) & \quad 22\,\% \\
            \end{tabular}
    \end{minipage}
\end{titlepage}


% Table of Contents
\newpage
\tableofcontents
\newpage

\section{Workflow and Team Contributions}
\subsection{Team Member Responsibilities}
\begin{itemize}
    \item \textbf{Michal Repčík (xrepcim00)}: 
    \begin{itemize}
        \item Designed and implemented the FSM for lexical analysis (Lexer)
        \item Designed and implemented the Abstract Syntax Tree (AST)
        \item Implemented the parser (excluding expression parsing)
    \end{itemize}
    \item \textbf{Alex Marinica (xmarina00)}: 
    \begin{itemize}
        \item Designed and implemented symbol table
        \item Designed and implemented semantic analysis 
    \end{itemize}
    \item \textbf{Šimon Bobko (xbobkos00)}: 
    \begin{itemize}
        \item Defined the language grammar
        \item Implemented the LL parsing table
        \item Developed the expression parsing module
        \item Written test cases for semantic analysis
    \end{itemize}
    \item \textbf{Martin Kandera (xkande00)}: 
    \begin{itemize}
        \item Designed and implemented code generation
        \item Written test cases for code generation
    \end{itemize}
\end{itemize}

The points are allocated based on each individual's participation and performance (the results from the first two test submissions). This is why members xrepcim00 and xbobkos00 received more points. 

\subsection{Workflow}
\begin{itemize}
    \item \textbf{Week 1 - 2}: 
    \begin{itemize}
        \item Task organization, project scheduling, and studying task requirements
        \item Token, FSM and lexer design and implementation
    \end{itemize}
    \item \textbf{Week 3 - 4}:
    \begin{itemize}
        \item Lexer optimization and debugging
        \item AST design and node creation
    \end{itemize}
    \item \textbf{Week 5 - 6}:
    \begin{itemize}
        \item AST and parser implementation (including expression parser)
        \item Starting on semantic analysis and code generation
    \end{itemize}
    \item \textbf{Week 7 - 9}:
    \begin{itemize}
        \item Finishing and merging all modules
        \item Testing and debugging completed compiler
    \end{itemize}
\end{itemize}

\newpage

% Error chapter
\section{Error Handling}
To manage errors, we added an external variable, \texttt{error\_tracker}, to track the current error state, and a \texttt{set\_error} function to update the tracker. This function only updates if the current state is \texttt{NO\_ERROR}, preventing overwriting previous errors. Error states are predefined in the \texttt{ErrorType} enumeration.

Details on error tracking can be found in \texttt{error.c} and \texttt{error.h}.

% Lexer Chapter
\section{Lexer\Grammar{lexer}}

\subsection{Overview}
The lexer (or scanner) is the first compiler stage, converting raw source code into tokens for the parser. It operates in sync with the parser, invoked on-demand to optimize memory use and detect syntax errors faster.

\subsection{Finite State Machine Representation}
The lexer uses a Finite-State Machine (FSM) to identify patterns in the source code. Each state represents a step in token recognition, with transitions driven by character inputs.

The FSM diagram in higher resolution can be viewed at the following link:  

\href{https://shorturl.at/rr1NE}{https://shorturl.at/rr1NE}.

\begin{center}
    \includegraphics[width=\textwidth]{lexer_fsm.jpg} % Replace with your FSM image
\end{center}

\subsection{Tokens}
The lexer converts lexemes into tokens, each comprising a type and optionally a value. Most tokens avoid allocating memory for values unless necessary, improving efficiency.

For example, given the source code:
\begin{verbatim}
    var x : ?i32 = 10;
\end{verbatim}

The lexer generates:

\begin{table}[h!]
\centering
\setlength{\arrayrulewidth}{0.2mm} % Adjust line thickness
\setlength{\tabcolsep}{6pt} % Adjust column padding
\renewcommand{\arraystretch}{1.3} % Adjust row height
\begin{tabular}{|p{3cm}|p{5cm}|p{4cm}|} % Adjust column widths
\hline
\textbf{Lexeme} & \textbf{Token Type} & \textbf{Token Value} \\ \hline
\texttt{var}    & \texttt{TOKEN\_VAR}          & \texttt{NULL}         \\ \hline
\texttt{x}      & \texttt{TOKEN\_IDENTIFIER}   & \texttt{"x"}          \\ \hline
\texttt{:}      & \texttt{TOKEN\_COLON}        & \texttt{NULL}         \\ \hline
\texttt{?}      & \texttt{TOKEN\_Q\_MARK}      & \texttt{NULL}         \\ \hline
\texttt{i32}    & \texttt{TOKEN\_I32}          & \texttt{NULL}         \\ \hline
\texttt{=}      & \texttt{TOKEN\_ASSIGN}       & \texttt{NULL}         \\ \hline
\texttt{10}     & \texttt{TOKEN\_INT}          & \texttt{"10"}         \\ \hline
\texttt{;}      & \texttt{TOKEN\_SEMICOLON}    & \texttt{NULL}         \\ \hline
\end{tabular}
\caption{Example of Tokens Generated by the Lexer}
\label{table:tokens}
\end{table}

Detailed token implementation can be found in \texttt{token.c} and \texttt{token.h} files.

\subsection{Optimization}
The lexer uses a keyword hash table and an ASCII lookup table for speed and efficiency.

\subsubsection{Keyword Hash Table}
The hash table distinguishes keywords from identifiers. It uses the djb2 hash function for quick lookups with perfect hashing. This design is scalable and outperformed alternatives in speed tests. 

Implementation details are in \texttt{keyword\_htab.c} and \texttt{keyword\_htab.h}.

\subsubsection{ASCII Lookup Table}
A stack-based ASCII lookup table validates characters after keywords, identifiers, or terms. With only 17 elements, lookups have \( O(1) \) complexity, reducing overhead. 

Implementation details are in \texttt{ascii\_lookup.c} and \texttt{ascii\_lookup.h}.

\newpage

\subsection{Lexer Architecture}
The lexer consists of the \texttt{Lexer} structure and \texttt{get\_token} function. The \texttt{Lexer} structure includes:

\begin{itemize}
    \item \texttt{src} – Pointer to the input source (file or \texttt{stdin}).
    \item \texttt{ascii\_l\_table} – The ASCII lookup table.
    \item \texttt{keyword\_htab} – The keyword hash table.
    \item \texttt{buff} – A dynamic string storing token values.
    \item \texttt{buff\_len} – Maximum buffer length for memory management.
\end{itemize}

\texttt{get\_token} processes the source code using the FSM, creating tokens or setting an error state if tokenization fails.

Details are in \texttt{lexer.c} and \texttt{lexer.h}.

% Parser Chapter
\section{Parser \cite{parser}}
The parser (syntax analyzer) ensures token sequences match the language grammar.

Our parser uses recursive descent, retrieving tokens with \texttt{advance\_token} and validating them with \texttt{check\_token}. Since the grammar is relatively simple, the parser tracks only a single instance of the current token, occasionally storing token values when necessary for Abstract Syntax Tree (AST) node creation.

Details are in \texttt{parser.c} and \texttt{parser.h}.

\subsection{Grammar \cite{grammar}}
The grammar for the language was initially defined in Extended Backus-Naur Form (EBNF), which contains more expressive constructs and is easier to read. The implementation can be found on the next page or 
\href{https://github.com/rm-a0/ifj-compiler/blob/main/doc/grammar\_edited.go}{here}.

This form of grammar was used to implement the parser. Later, it was rewritten into standard Backus-Naur Form (BNF) for the purpose of constructing the LL table.

\newpage

\begin{tabbing}
\hspace{-1cm} \= \hspace{4cm} \= \hspace{1cm} \= \kill
\> \texttt{<program>} \> ::= \> \texttt{<prolog>} \mid \texttt{<top\_level\_decl>}* \\
\> \texttt{<top\_level\_decl>} \> ::= \> \texttt{<fn\_decl>} \\
\> \texttt{<identifier>} \> ::= \> \texttt{[a-zA-Z] [_a-zA-Z0-9]*} \\
\> \texttt{<integer>} \> ::= \> \texttt{[0-9]+} \\
\> \texttt{<float>} \> ::= \> \texttt{<integer>} "." \texttt{[("e" | "E") ("+" | "-")]} \texttt{<integer>} \\
\> \texttt{<string\_literal>} \> ::= \> \texttt{[^]*} \\
\> \texttt{<data\_type>} \> ::= \> \texttt{["?"] ("void" | "u8" | "i32" | "f64" | "[]u8")} \\
\> \texttt{<fn\_decl>} \> ::= \> \texttt{"pub"} \texttt{"fn"} \texttt{<identifier>} "(" [\texttt{<param\_list>}]) ")" \\
\> \> \texttt{<data\_type>} \texttt{<block>} \\
\> \texttt{<param\_list>} \> ::= \> \texttt{<param>} \texttt{("," <param>)*} \\
\> \texttt{<param>} \> ::= \> \texttt{<identifier>} ":" \texttt{<data\_type>} \\
\> \texttt{<const\_decl>} \> ::= \> \texttt{"const"} \texttt{<identifier>} [\texttt{":" <data\_type>}] "=" \texttt{<expression>} ";" \\
\> \texttt{<var\_decl>} \> ::= \> \texttt{"var"} \texttt{<identifier>} [\texttt{":" <data\_type>}] "=" \texttt{<expression>} ";" \\
\> \texttt{<expression>} \> ::= \> \texttt{<term>} \texttt{(("+" | "-") <term>)*} \\
\> \texttt{<term>} \> ::= \> \texttt{<factor>} \texttt{(("*" | "/") <factor>)*} \\
\> \texttt{<factor>} \> ::= \> \texttt{<number>} | \texttt{<identifier>} | \texttt{<string\_literal>} | "(" \texttt{<expression>} ")" \\
\> \texttt{<number>} \> ::= \> \texttt{<integer>} | \texttt{<float>} \\
\> \texttt{<block>} \> ::= \> \texttt{"{" <statement>* "}"} \\
\> \texttt{<statement>} \> ::= \> \texttt{<identifier> <statement\_suffix>} | \texttt{<const\_decl>} | \texttt{<var\_decl>} \\
\> \> \texttt{|} \texttt{<if\_statement>} | \texttt{<while\_statement>} | \texttt{<return\_statement>} \\
\> \texttt{<statement\_suffix>} \> ::= \> \texttt{"=" <expression> ";"} \mid \texttt{"(" [<arg\_list>] ")";} \\
\> \texttt{<assignment>} \> ::= \> \texttt{<identifier>} "=" \texttt{<expression>} ";" \\
\> \texttt{<fn\_call>} \> ::= \> \texttt{<fn\_identifier>} "(" [\texttt{<arg\_list>}]) ")" \\
\> \texttt{<fn\_call\_statement>} \> ::= \> \texttt{<fn\_call>} ";" \\
\> \texttt{<fn\_identifier>} \> ::= \> \texttt{<built\_in\_fn>} | \texttt{<identifier>} \\
\> \texttt{<arg\_list>} \> ::= \> \texttt{<expression>} \texttt{("," <expression>)*} \\
\> \texttt{<built\_in\_fn>} \> ::= \> \texttt{"ifj" "."} \\
\> \> \texttt{"write" | "readstr" | "readi32" | "readf64" |} \\
\> \> \texttt{"i2f" | "f2i" | "string" | "length" |} \\
\> \> \texttt{"concat" | "substring" | "strcmp" | "ord" | "chr"} \\
\> \texttt{<if\_statement>} \> ::= \> \texttt{"if"} "(" \texttt{<expression>} [\texttt{<bin\_expression>}]) ")" \\
\> \> \texttt{[<element\_bind>]} \texttt{<block>} \texttt{"else"} \texttt{<block>} \\
\> \texttt{<while\_statement>} \> ::= \> \texttt{"while"} "(" \texttt{<expression>} [\texttt{<bin\_expression>}]) ")" \\
\> \> \texttt{[<element\_bind>]} \texttt{<block>} \\
\> \texttt{<bin\_expression>} \> ::= \> \texttt{<binary\_operator>} \texttt{<expression>} \\
\> \texttt{<element\_bind>} \> ::= \> \texttt{"|"} \texttt{<identifier>} "|" \\
\> \texttt{<return\_statement>} \> ::= \> \texttt{"return"} [\texttt{<expression>}] ";" \\
\> \texttt{<binary\_operator>} \> ::= \> \texttt{"=="} \mid \texttt{"!="} \mid \texttt{"<="} \mid \texttt{">="} \mid \texttt{">"} \mid \texttt{"<"}
\end{tabbing}

\vspace{1cm}
\begin{center}
    \caption{Grammar in Extended Backus-Naur From (EBNF)}
\end{center}

\newpage
\section*{LL Parsing Table}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{LLtable.png} % Replace with your image file name
    \caption{LL Parsing Table}
    \label{fig:ll_table}
\end{figure}

\subsection{Abstract Syntax Tree \cite{ast}}
The AST was designed as a collection of nodes, each representing a specific part of the code. Our AST nodes are less modular than those in typical compilers, but given the simple and immutable grammar, this design choice was practical and effective.

Below is a simple example of an AST for this code:
\begin{verbatim}
    pub fn main (x : ?i32) void {
        if (x) |temp| {
            var y = temp;
            return;
        }
    } 
\end{verbatim}
\textit{Note: This code is intentionally not semantically correct and serves only as a simplified example.}

\begin{center}
    \includegraphics[width=1\linewidth]{ast_design.png}
\end{center}

Complete AST implementation can be found in \texttt{ast.c} and \texttt{ast.h} files.

\subsection{Expression Parser}
The expression parser implementation is inspired by the precedence syntactical analysis. Expressions are parsed based on operator precedence, which is defined in the \texttt{get\_precedence()} function. It uses two stacks: an operator stack, which holds AST \texttt{OperatorType}-s, and an operand stack, which holds \texttt{ASTNode}-s. Additionally, the parser supports both user-defined and built-in function calls as part of expressions.

Implementation of expression parser can be found within \texttt{parser.c} and \texttt{parser.h} files. Stack implementation can be found in \texttt{stack\_exp.h}, \texttt{stack\_exp.c}, \texttt{ast\_node\_stack.h} and \texttt{ast\_node\_stack.c} files.

\begin{table}[ht]
\centering
\begin{tabular}{|>{\raggedright\arraybackslash}p{6cm}|p{3cm}|}
\hline
\textbf{Operator} & \textbf{Precedence} \\
\hline
TOKEN\_EQU, TOKEN\_NOT\_EQU & 1 \\
\hline
TOKEN\_LESS, TOKEN\_LESS\_EQU, TOKEN\_GREATER, TOKEN\_GREATER\_EQU & 2 \\
\hline
TOKEN\_PLUS, TOKEN\_MINUS & 3 \\
\hline
TOKEN\_MULT, TOKEN\_DIV & 4 \\
\hline
\end{tabular}
\caption{Operator Precedence Table}
\end{table}

\section{Semantic Analysis\cite{djb2}}

The implementation of semantic analysis in this project employs semi-dynamic allocation of resources, where the symbol table uses predefined values for initial capacities and resizes dynamically as needed. The semantic analyzer relies on a recursive descent approach to traverse the Abstract Syntax Tree (AST) and perform semantic checks on various language constructs.

The \textbf{symbol table (symtable.c)} is central to the semantic analysis. It uses a hash table with the djb2 algorithm for efficient symbol storage and retrieval. Each symbol is stored as either a \texttt{VarSymbol} or a \texttt{FuncSymbol}, depending on whether it represents a variable or a function. Variables are characterized by attributes such as type, nullability, value, and usage flags. Functions include details about parameters, return type, and scope stack. The hash table expands when the load factor reaches a predefined threshold (0.75), ensuring that lookups remain efficient even as the number of entries grows.

To manage scopes, the scope stack (\texttt{stack.c}) is implemented as a dynamic array of frames. Each frame contains a local symbol table that represents a distinct block. The stack provides operations to push and pop frames, facilitating nested scopes, and supports efficient lookups by traversing from the top frame downward until a matching symbol is found.

The semantic analysis process begins by populating the global symbol table with function declarations extracted from \texttt{AST\_PROGRAM}. This step ensures that functions can be used before their declarations are encountered in the AST, aligning with the assignment requirements. The global table allows deferred semantic checks, where actual validation of function bodies occurs only when the function is invoked. This optimization avoids unnecessary evaluation of unused functions, focusing semantic checks on parts of the program that affect its execution.

The analyzer ensures type compatibility, allowing null assignments only for explicitly nullable variables and permitting limited implicit conversions, like integers to floats.

The analysis of expressions involves evaluating operator types, operand compatibility, and nullability. Relational and arithmetic operators require specific type combinations, and implicit conversions are allowed only for literals. The analyzer enforces semantic correctness by raising errors for unsupported operations or mismatched types.

Function calls are validated by checking argument counts, types, and return statements. Built-in functions follow predefined rules, while user-defined functions are analyzed only when invoked, ensuring correctness and flexibility.


\newpage
% Code Generator Chapter
\section{Code Generator}

\subsection{Overview}
The code generator converts the Abstract Syntax Tree (AST) into intermediate IFJcode24, executable by a virtual machine. It supports constructs like variable declarations, expressions, loops, conditionals, and function calls, ensuring all frames remain local.

Implementation of code generator can be found in \texttt{generator.c} and \texttt{generator.h} files. Helper function for generator can be found in \texttt{generator\_instructions.c} and \\
\texttt{generator\_instructions.h} files.

\subsection{Structure and Components}
Implemented in \texttt{generator.c} and \texttt{generator\_instructions.c}, the generator relies on:
\begin{itemize}
    \item \textbf{Local Frame Array}: Tracks variables in a dynamically managed local frame.
    \item \textbf{While Stack}: Handles nested \texttt{while} loops with unique labels and counters.
    \item \textbf{Temporary Counter}: Generates unique names for variables and labels.
\end{itemize}

\subsection{Workflow}
\begin{enumerate}
    \item \textbf{Initialization}: Prepare data structures (local frame, \texttt{while} stack).
    \item \textbf{AST Traversal}: Process nodes recursively, invoking routines for specific constructs.
    \item \textbf{Instruction Generation}: Emit instructions for variables, control flow, expressions, and function calls.
    \item \textbf{Resource Cleanup}: Clear local frames and loop stack after processing blocks.
\end{enumerate}

\subsection{Features}
\begin{itemize}
    \item \textbf{Variable Management}: Variables are declared with \texttt{DEFVAR} in dynamically scoped local frames.
    \item \textbf{Control Flow}: Constructs like \texttt{if-else} and \texttt{while} are implemented using labels and stack-based nesting.
    \item \textbf{Expressions and Operations}: Arithmetic and logical expressions use stack-based evaluation with instructions like \texttt{PUSHS}, \texttt{ADDS}, and \texttt{POPS}.
    \item \textbf{Function Calls}: Functions create a new local frame, and their execution context is restored on return.
\end{itemize}

\subsection{Error Handling}
Errors are limited to:
\begin{itemize}
    \item Memory allocation failures for local frame or loop stack.
    \item A \texttt{NULL} AST root passed to \texttt{generate\_code}.
    \item Encountering unsupported AST nodes during traversal.
\end{itemize}

\printbibliography
% [Rest of the code remains the same]
\end{document}
